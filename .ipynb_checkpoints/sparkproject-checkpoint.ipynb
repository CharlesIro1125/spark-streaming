{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import asc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import StringType,DateType,DataType,datetime,TimestampType\n",
    "from pyspark.sql.types import IntegerType,FloatType,datetime,DatetimeConverter\n",
    "from pyspark.sql.functions import desc,col\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import asc,unix_timestamp, from_unixtime\n",
    "from pyspark.streaming import StreamingListener\n",
    "from pyspark.sql import Window\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "import pyspark.sql.types as tp\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from dateutil import parser\n",
    "import iso8601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If spark is to be run in local mode the setMaster has to be \"local\" or the IP address of the master computer if its to be run in a distributed node.\n",
    "For local node, the number of thread (proccessors) can also be set with \"local[2]\" for 2 proccessors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a configuration setting for the spark object\n",
    "configuration = SparkConf().setAppName('spark streaming').setMaster('local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparkcontext is used to create  RDD objects on the clusters. RDD objects are Resilient distributed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a sparkcontext\n",
    "sc= SparkContext(conf=configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Sparksession is used to create Dataframes object, most pandas dataframe method can be applied on the spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing a sparksession\n",
    "spark = SparkSession \\\n",
    "         .builder \\\n",
    "         .appName(\"spark streaming\") \\\n",
    "         .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.id', 'local-1599026175002'),\n",
       " ('spark.driver.port', '4148'),\n",
       " ('spark.driver.host', 'DESKTOP-JG0SMAD'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'spark streaming'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying the configuration of the sparkcontext using a sparksession object\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to set the schema of the dataframe to be streamed. ommiting this will cause the sparksession read method to first\n",
    "#deduce the schema of the input before starting its read process. this takes additional time, so collectig the schema  \n",
    "# information ahead makes the read method faster\n",
    "\n",
    "source = \"file:///Users/iroch/dev/toprocess\"\n",
    "\n",
    "data = spark.read.load(\"C:/Users/iroch/dev/process/household_data_15min_singleindex.csv\",format=\"csv\", sep=\",\",\n",
    "                     header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script below has two section, the database connection function and the streaming process. the streaming process uses a foreachBatch method to output the streaming job to a sink. the ForeachBatch method accepts a function that is written to do a connection to a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to do a connection to a database after recieving the dataframe streams in batches\n",
    "\n",
    "def database_conn(datafrm,batchId):\n",
    "    # This function performs database_connection when called, it collects some batch of the streaming data and performs a connection to \n",
    "    # the batabase. it also collects the batch id of the stream as well.\n",
    "    # it performs methods like database open and close using a try and catch exception.\n",
    "    \n",
    "    db_p = \"C:/here/the/path_to/the_database/goes/db.sqlite3\"\n",
    "    dlist = datafrm.collect()\n",
    "    df =pd.DataFrame(data=dlist,columns=[\"utc_timestamp\",'dishwasher','electric_vehicle','freezer','grid_export','grid_import',\\\n",
    "                              'heat_pump','pv','refrigerator','washing_machine','total_load_consume'])\n",
    "    \n",
    "\n",
    "    \n",
    "    try:\n",
    "        conn = None\n",
    "        conn = sqlite.connect(db_p)\n",
    "            \n",
    "        c = conn.cursor()\n",
    "        for i,row in df.iterrows():\n",
    "            \n",
    "            # the data are inserted by rows\n",
    "            \n",
    "            row[\"utc_timestamp\"]= datetime.strptime(row[\"utc_timestamp\"],'%Y-%m-%dT%H:%M:%SZ')\n",
    "            \n",
    "            val =(row[\"utc_timestamp\"],row[\"dishwasher\"],row['electric_vehicle'],row['freezer'],row['grid_export'],\\\n",
    "                  row['grid_import'],row['heat_pump'],row['pv'],row['refrigerator'],row['washing_machine'],row['total_load_consume'])\n",
    "            \n",
    "            sql = \"\"\"insert into forecast_fcast (utc_timestamp,dishwasher,electric_vehicle,freezer,grid_export,grid_import,heat_pump,pv,\\\n",
    "                    refrigerator,washing_machine,total_load_consume) values (?,?,?,?,?,?,?,?,?,?,?)\"\"\"\n",
    "            \n",
    "            \n",
    "            c.execute(sql,val)\n",
    "            conn.commit()\n",
    "    \n",
    "        \n",
    "        \n",
    "    except Error:\n",
    "        print('failed to connect')\n",
    "    finally:\n",
    "        if conn:\n",
    "            c.close()\n",
    "            conn.close()\n",
    "\n",
    "            \n",
    "            \n",
    "# begin the streaming with info of the source to listen to for data, the schema of the expected data and to use the column\n",
    "#names.\n",
    "dstream = spark.readStream.csv(path=source,schema=data.schema, header=True)\n",
    "\n",
    "\n",
    "# select the specified columns from the input stream\n",
    "\n",
    "maincol = dstream.select(\"utc_timestamp\",\"cet_cest_timestamp\",\"DE_KN_residential4_dishwasher\",\"DE_KN_residential4_ev\",\n",
    " \"DE_KN_residential4_freezer\",\"DE_KN_residential4_grid_export\",\"DE_KN_residential4_grid_import\",\n",
    " \"DE_KN_residential4_heat_pump\",\"DE_KN_residential4_pv\",\"DE_KN_residential4_refrigerator\",\"DE_KN_residential4_washing_machine\")\n",
    "\n",
    "\n",
    "# change the data type as suitable for the task\n",
    "\n",
    "maincol = maincol.withColumn(\"time_stamp\",maincol[\"utc_timestamp\"].cast(DateType()))\n",
    "maincol = maincol.withColumn(\"dishwasher\",maincol[\"DE_KN_residential4_dishwasher\"].cast(FloatType()))\n",
    "maincol = maincol.withColumn(\"electric_vehicle\",maincol[\"DE_KN_residential4_ev\"].cast(FloatType()))\n",
    "maincol = maincol.withColumn(\"freezer\",maincol[\"DE_KN_residential4_freezer\"].cast(FloatType()))\n",
    "maincol = maincol.withColumn(\"grid_export\",maincol[\"DE_KN_residential4_grid_export\"].cast(FloatType()))\n",
    "maincol = maincol.withColumn(\"grid_import\",maincol[\"DE_KN_residential4_grid_import\"].cast(FloatType()))\n",
    "maincol = maincol.withColumn(\"heat_pump\",maincol[\"DE_KN_residential4_heat_pump\"].cast(FloatType()))\n",
    "maincol = maincol.withColumn(\"pv\",maincol[\"DE_KN_residential4_pv\"].cast(FloatType()))\n",
    "maincol = maincol.withColumn(\"refrigerator\",maincol[\"DE_KN_residential4_refrigerator\"].cast(FloatType()))\n",
    "maincol = maincol.withColumn(\"washing_machine\",maincol[\"DE_KN_residential4_washing_machine\"].cast(FloatType()))\n",
    "\n",
    "\n",
    "# fill in missing values with float 0.00\n",
    "\n",
    "maincol2 = maincol.na.fill(0.00, subset=['dishwasher','electric_vehicle','freezer','grid_export','grid_import',\\\n",
    "                              'heat_pump','pv','refrigerator','washing_machine'])\n",
    "\n",
    "# perform addition operation on the loads to get a total load used in the resident\n",
    "\n",
    "main1 =maincol2.withColumn('total_1',col('electric_vehicle')+col('dishwasher')+col('freezer'))\n",
    "main2 =main1.withColumn('total_2',col('total_1')+col('refrigerator')+col('washing_machine'))\n",
    "main3 =main2.withColumn('total_load_consume',col('total_2')+col('heat_pump'))\n",
    "\n",
    "\n",
    "#drop unimportant columns from the stream\n",
    "\n",
    "Dmain = main3.drop('total_1','total_2',\"cet_cest_timestamp\",\"DE_KN_residential4_ev\",\"DE_KN_residential4_dishwasher\",\\\n",
    "                   \"DE_KN_residential4_freezer\",\"DE_KN_residential4_grid_export\",\"DE_KN_residential4_grid_import\",\"DE_KN_residential4_heat_pump\",\\\n",
    "                  \"DE_KN_residential4_pv\",\"DE_KN_residential4_refrigerator\",\"DE_KN_residential4_washing_machine\",\"time_stamp\")\n",
    "\n",
    "# write the datastream to a batabase using the provided function. write the checkpoint information to a secured location in \n",
    "#case of failures during streaming. this info can be used to restore the streaming process. retrieve the next\n",
    "#data from source every 200 seconds with the trigger, then start the streaming without termination.\n",
    "\n",
    "query = Dmain.writeStream.foreachBatch(database_conn).outputMode(\"append\").option(\"checkpointLocation\", \"C:/Users/iroch/dev/result/checkpoint\")\\\n",
    "            .trigger(processingTime ='200 seconds').start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the streaming process is running\n",
    "\n",
    "Dmain.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '409fa97a-038e-4a97-8898-1bc6d3f9cdfc', 'runId': 'de08df24-76e0-49c0-8c4a-5a9ca1e596c1', 'name': None, 'timestamp': '2020-09-02T06:09:33.048Z', 'batchId': 6, 'numInputRows': 0, 'processedRowsPerSecond': 0.0, 'durationMs': {'getOffset': 66, 'triggerExecution': 365}, 'stateOperators': [], 'sources': [{'description': 'FileStreamSource[file:/Users/iroch/dev/toprocess]', 'startOffset': {'logOffset': 5}, 'endOffset': {'logOffset': 5}, 'numInputRows': 0, 'processedRowsPerSecond': 0.0}], 'sink': {'description': 'ForeachBatchSink'}}\n"
     ]
    }
   ],
   "source": [
    "# print the last progress\n",
    "\n",
    "print(query.lastProgress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Waiting for next trigger', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# print the status\n",
    "\n",
    "print(query.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
